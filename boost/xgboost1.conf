#/home/whale/Documents/xgboost/xgboost
# General Parameters, see comment for each definition
# can be gbtree or gblinear
booster = gbtree 
#silent = 0
nthread = 6
# choose logistic regression loss function for binary classification
objective = binary:logistic
#base_score = 0.5
eval_metric=logloss
#seed = 0

# Tree Booster Parameters
# step size shrinkage
eta = 0.3 
# minimum loss reduction required to make a further partition
gamma = 1.0 
# minimum sum of instance weight(hessian) needed in a child
min_child_weight = 1 
# maximum depth of a tree
max_depth = 10 
#subsample = 1
#colsample_bytree = 1

# Task Parameters
#use_buffer = 1
# the number of round to do boosting
num_round = 500
# 0 means do not save any model except the final round model
save_period = 100 
# The path of training data
data = "train3day.txt" 
# The path of validation data, used to monitor training process, here [test] sets name of the validation set
#eval[train] = "train9days.xgb"
#eval[valid] = "train10thday1.txt" 
# The path of test data 
test:data = "test.txt"  

task = pred
model_in = 0500.model
#model_out = train9days.xgb.model
#model_dir = xgboostmodels
name_pred = xgboost1.txt
